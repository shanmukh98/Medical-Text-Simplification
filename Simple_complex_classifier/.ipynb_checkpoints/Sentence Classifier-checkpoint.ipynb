{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../../Medical-Text-Simplification./Dataset/sentence-aligned.v2/complex-simple.txt\"\n",
    "f =open(path,\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_punct(x):\n",
    "    return re.sub(r\"[!\\\"#$%&\\'()*+,-./:;<=>?@[\\]^_`{|}~\\n\\tâ€“\\\\]\",\" \",x).lower()\n",
    "def strip_non_ascii(x):\n",
    "    return re.sub(r'[^\\x00-\\x7F]',' ',x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple = []\n",
    "compl = []\n",
    "sentences = []\n",
    "while True:\n",
    "    try:\n",
    "        sent = f.readline().split(\"\\t\")\n",
    "    except:\n",
    "        break\n",
    "#     print (sent)\n",
    "    if len(sent) == 1:\n",
    "        break\n",
    "    sentences.append(strip_non_ascii(strip_punct(sent[0])).split())\n",
    "    sentences.append(strip_non_ascii(strip_punct(sent[1][:-1])).split())\n",
    "\n",
    "    if sent[0]==sent[1][:-1]:\n",
    "            \n",
    "        simple.append(strip_non_ascii(strip_punct(sent[0])).split())\n",
    "        compl.append(strip_non_ascii(strip_punct(sent[1][:-1])).split())\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "        \n",
    "# model = Word2Vec(sentences, min_count=2,workers=4,size=25)\n",
    "model = gensim.\n",
    "# len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50352\n",
      "50352\n",
      "[['it', 'is', 'the', 'county', 'seat', 'of', 'alfalfa', 'county'], ['it', 'is', 'the', 'county', 'seat', 'of', 'alfalfa', 'county'], ['cherokee', 'is', 'a', 'city', 'of', 'oklahoma', 'in', 'the', 'united', 'states'], ['cherokee', 'is', 'a', 'city', 'in', 'alfalfa', 'county', 'oklahoma', 'united', 'states'], ['skateboard', 'decks', 'are', 'normally', 'between', '28', 'and', '33', 'inches', 'long'], ['skateboard', 'decks', 'are', 'usually', 'between', '28', 'and', '33', 'inches', 'long'], ['the', 'bottom', 'of', 'the', 'deck', 'can', 'be', 'printed', 'with', 'a', 'design', 'by', 'the', 'maker', 'or', 'it', 'can', 'be', 'blank'], ['the', 'underside', 'of', 'the', 'deck', 'can', 'be', 'printed', 'with', 'a', 'design', 'by', 'the', 'manufacturer', 'blank', 'or', 'decorated', 'by', 'any', 'other', 'means'], ['the', 'longboard', 'was', 'made', 'by', 'two', 'surfers', 'ben', 'whatson', 'and', 'jonny', 'drapper'], ['this', 'was', 'created', 'by', 'two', 'surfers', 'ben', 'whatson', 'and', 'jonny', 'drapper']]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-de4825003593>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "print (len(simple))\n",
    "print (len(compl))\n",
    "print (sentences[0:10])\n",
    "sentences = list(set(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = []\n",
    "for i in range(len(simple)):\n",
    "    lens.append(len(simple[i]))\n",
    "    lens.append(len(compl[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141\n"
     ]
    }
   ],
   "source": [
    "max_len = max(lens)\n",
    "print (max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=107136, size=25, alpha=0.025)\n",
      "[ 1.4927553  -0.8425305   0.49155968 -0.92900306 -0.4976271   1.3914464\n",
      " -0.2655331  -1.2778274   0.772134   -0.09971397  0.08382952 -0.26104328\n",
      " -0.24953038 -1.1547369   0.57630414 -1.6127458   0.38493362  1.7987732\n",
      " -0.56433773 -0.8102146   0.34215942  0.8559468   0.04438318 -0.12259465\n",
      " -0.56960624]\n",
      "11.846261\n",
      "[ 0.12272041 -0.35937375 -0.49986514  0.3780573   0.02328784  1.7220805\n",
      "  0.096172   -0.67056155 -0.63640606 -0.10037097  0.50441    -0.02892712\n",
      "  0.06831259 -0.36061954  0.18908845 -1.0266637   0.08308092  0.5167506\n",
      " -0.10620487 -0.7173645   0.01509012  0.3186081   0.10671178  0.15312755\n",
      " -0.02593602]\n",
      "[-0.07962865 -2.450912   -0.7024285  -0.19444595 -0.21969117  1.2737824\n",
      " -0.01834635 -0.31152093 -1.7032655  -0.7604355  -0.984194   -0.55185455\n",
      "  0.084891    0.6373903  -0.46952298 -1.6319882  -2.0125356   0.07191981\n",
      "  0.588454   -0.05584337  0.82396114  0.63338566  1.0122058  -0.09871181\n",
      " -1.8405694 ]\n",
      "142.85568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "# summarize vocabulary\n",
    "# words = list(model.wv.vocab)\n",
    "# print(words)\n",
    "# access vector for one word\n",
    "print(model['sentence'])\n",
    "print((np.asarray(model['sentence']-model[\"sentences\"])**2).sum())\n",
    "print(model[\"sentences\"])\n",
    "print(model[\"cat\"])\n",
    "print((np.asarray(model['usa']-model[\"america\"])**2).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.80135107 -2.17025805  5.72633314 ...  0.          0.\n",
      "  0.        ]\n",
      "[ 3.80135107 -2.17025805  5.72633314 ...  0.          0.\n",
      "  0.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "Y = np.ones((2*len(simple),1))\n",
    "for i in range(len(simple)):\n",
    "    temp_x = np.zeros((max_len*25))\n",
    "    Y[i] = 0\n",
    "    for j in range(len(simple[i])):\n",
    "        temp_x[j*25:(j+1)*25] = np.asarray(model[simple[i][j]])\n",
    "#         print (simple[i][j])\n",
    "    print (temp_x)\n",
    "    break\n",
    "    \n",
    "    X.append(temp_x)\n",
    "for i in range(len(compl)):\n",
    "    temp_x = np.zeros((max_len*25))\n",
    "    for j in range(len(simple[i])):\n",
    "        temp_x[j*25:(j+1)*25] = np.asarray(model[compl[i][j]])\n",
    "    print (temp_x)\n",
    "    break\n",
    "#         print (simple[i][j])\n",
    "    X.append(temp_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-7629f73d288c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "# X = np.asarray(X)\n",
    "print (len(X))\n",
    "for i in range(25*max_len):\n",
    "    X[:,i] = (X[:,i]-np.mean(X[:,i]))/np.var(X[:,i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=100,whiten=True)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = pca.transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(X_,Y,test_size = 0.2)\n",
    "clf = KNeighborsClassifier()\n",
    "clf.fit(X,Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
